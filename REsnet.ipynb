{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d5428f6-2574-41f2-bd90-215675424979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.1\n",
      "Keras version: 3.3.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Ensure TensorFlow and Keras versions\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {tf.keras.__version__}\")\n",
    "\n",
    "# Load the ResNet50 model without the top (fully connected layers)\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39802ee5-8536-4c7d-ae01-bb93851ac415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the ResNet50 layers so they are not trained\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bad2f93-44cf-4bd8-a8af-fb9c7af66c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add custom classification layers\n",
    "model = Sequential([\n",
    "    Input(shape=(224, 224, 3)),  # Explicitly add an Input layer\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(3, activation='softmax')  # 3 classes: image1, image2, image3\n",
    "])\n",
    "\n",
    "# Learning rate scheduler function\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 5:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * 0.1\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "349bcd06-7b11-4285-8497-ec9bac5ed0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7249 images belonging to 3 classes.\n",
      "Found 1858 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation for training set\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    brightness_range=[0.8, 1.2]\n",
    ")\n",
    "\n",
    "# Data generator for reading and augmenting data from directories\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    r\"C:\\Users\\hp\\Documents\\Miniproject\\trail\\train\",\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Data generator for validation/test set (only rescaling)\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    r\"C:\\Users\\hp\\Documents\\Miniproject\\trail\\test\",\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "130d64fb-0139-4b60-bf25-8fe25e9ee60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m665s\u001b[0m 3s/step - accuracy: 0.5618 - loss: 1.2009 - val_accuracy: 0.8385 - val_loss: 0.4893 - learning_rate: 1.0000e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m656s\u001b[0m 3s/step - accuracy: 0.7551 - loss: 0.6062 - val_accuracy: 0.8149 - val_loss: 0.5426 - learning_rate: 1.0000e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m645s\u001b[0m 3s/step - accuracy: 0.8283 - loss: 0.4809 - val_accuracy: 0.8245 - val_loss: 0.4266 - learning_rate: 1.0000e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m643s\u001b[0m 3s/step - accuracy: 0.8394 - loss: 0.4272 - val_accuracy: 0.8337 - val_loss: 0.4804 - learning_rate: 1.0000e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m646s\u001b[0m 3s/step - accuracy: 0.8759 - loss: 0.3619 - val_accuracy: 0.8945 - val_loss: 0.2758 - learning_rate: 1.0000e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m644s\u001b[0m 3s/step - accuracy: 0.8980 - loss: 0.3046 - val_accuracy: 0.9015 - val_loss: 0.2875 - learning_rate: 1.0000e-05\n",
      "Epoch 7/10\n",
      "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m645s\u001b[0m 3s/step - accuracy: 0.8990 - loss: 0.3039 - val_accuracy: 0.8821 - val_loss: 0.3398 - learning_rate: 1.0000e-06\n",
      "Epoch 8/10\n",
      "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m645s\u001b[0m 3s/step - accuracy: 0.8989 - loss: 0.2957 - val_accuracy: 0.8837 - val_loss: 0.3382 - learning_rate: 1.0000e-07\n",
      "Epoch 9/10\n",
      "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m645s\u001b[0m 3s/step - accuracy: 0.8960 - loss: 0.3023 - val_accuracy: 0.8837 - val_loss: 0.3373 - learning_rate: 1.0000e-08\n",
      "Epoch 10/10\n",
      "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m638s\u001b[0m 3s/step - accuracy: 0.8966 - loss: 0.3009 - val_accuracy: 0.8837 - val_loss: 0.3372 - learning_rate: 1.0000e-09\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=test_generator,\n",
    "    callbacks=[lr_scheduler]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38ffcae9-2f5c-4fc7-8198-94df8e26967f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 2s/step\n",
      "['class1', 'class2', 'class1', 'class1', 'class1', 'class1', 'class2', 'class2', 'class2', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class3', 'class1', 'class1', 'class1', 'class1', 'class2', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class2', 'class1', 'class2', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class2', 'class1', 'class1', 'class1', 'class2', 'class1', 'class1', 'class2', 'class1', 'class2', 'class3', 'class1', 'class1', 'class1', 'class1', 'class2', 'class2', 'class1', 'class1', 'class1', 'class2', 'class2', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class2', 'class1', 'class1', 'class1', 'class1', 'class1', 'class2', 'class2', 'class2', 'class1', 'class1', 'class2', 'class1', 'class1', 'class2', 'class1', 'class2', 'class1', 'class1', 'class1', 'class2', 'class2', 'class3', 'class2', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class2', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class2', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class2', 'class1', 'class1', 'class2', 'class3', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class2', 'class1', 'class2', 'class1', 'class2', 'class1', 'class1', 'class1', 'class1', 'class2', 'class2', 'class2', 'class3', 'class1', 'class2', 'class1', 'class1', 'class1', 'class1', 'class1', 'class2', 'class3', 'class2', 'class1', 'class2', 'class1', 'class1', 'class1', 'class3', 'class2', 'class1', 'class1', 'class1', 'class1', 'class2', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class2', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class3', 'class1', 'class2', 'class2', 'class1', 'class2', 'class1', 'class1', 'class2', 'class1', 'class2', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class2', 'class2', 'class1', 'class2', 'class1', 'class1', 'class1', 'class3', 'class2', 'class2', 'class1', 'class1', 'class2', 'class1', 'class2', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class2', 'class2', 'class2', 'class1', 'class1', 'class2', 'class1', 'class1', 'class1', 'class1', 'class1', 'class2', 'class1', 'class3', 'class1', 'class2', 'class1', 'class2', 'class1', 'class2', 'class1', 'class1', 'class2', 'class1', 'class1', 'class1', 'class1', 'class2', 'class1', 'class1', 'class1', 'class2', 'class2', 'class1', 'class2', 'class2', 'class1', 'class1', 'class1', 'class1', 'class3', 'class2', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class2', 'class1', 'class1', 'class1', 'class2', 'class2', 'class2', 'class1', 'class1', 'class1', 'class1', 'class2', 'class1', 'class1', 'class2', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class2', 'class2', 'class1', 'class2', 'class2', 'class1', 'class1', 'class2', 'class1', 'class1', 'class2', 'class1', 'class1', 'class1', 'class2', 'class1', 'class2', 'class1', 'class2', 'class1', 'class1', 'class2', 'class1', 'class1', 'class1', 'class2', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class2', 'class1', 'class2', 'class2', 'class2', 'class2', 'class2', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class2', 'class1', 'class1', 'class2', 'class2', 'class1', 'class1', 'class1', 'class2', 'class1', 'class1', 'class2', 'class1', 'class1', 'class1', 'class2', 'class1', 'class1', 'class1', 'class2', 'class2', 'class2', 'class2', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class2', 'class1', 'class1', 'class1', 'class2', 'class2', 'class1', 'class2', 'class1', 'class1', 'class2', 'class1', 'class1', 'class1', 'class2', 'class1', 'class1', 'class2', 'class3', 'class2', 'class1', 'class1', 'class1', 'class1', 'class3', 'class1', 'class1', 'class1', 'class2', 'class1', 'class1', 'class2', 'class1', 'class1', 'class2', 'class2', 'class1', 'class2', 'class1', 'class1', 'class1', 'class3', 'class1', 'class2', 'class3', 'class2', 'class2', 'class1', 'class1', 'class1', 'class1', 'class2', 'class3', 'class1', 'class1', 'class2', 'class2', 'class2', 'class1', 'class1', 'class1', 'class2', 'class1', 'class1', 'class1', 'class1', 'class2', 'class1', 'class1', 'class2', 'class1', 'class2', 'class1', 'class1', 'class2', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class2', 'class2', 'class1', 'class2', 'class2', 'class1', 'class2', 'class2', 'class1', 'class1', 'class2', 'class2', 'class1', 'class2', 'class1', 'class1', 'class1', 'class1', 'class1', 'class2', 'class1', 'class2', 'class1', 'class2', 'class2', 'class1', 'class1', 'class1', 'class2', 'class3', 'class1', 'class1', 'class1', 'class1', 'class1', 'class2', 'class1', 'class2', 'class1', 'class1', 'class1', 'class2', 'class1', 'class1', 'class1', 'class1', 'class3', 'class1', 'class1', 'class1', 'class1', 'class2', 'class2', 'class2', 'class1', 'class2', 'class2', 'class1', 'class2', 'class1', 'class1', 'class1', 'class2', 'class1', 'class2', 'class1', 'class1', 'class1', 'class2', 'class1', 'class1', 'class1', 'class1', 'class2', 'class1', 'class1', 'class1', 'class1', 'class3', 'class1', 'class1', 'class1', 'class1', 'class2', 'class2', 'class1', 'class1', 'class2', 'class2', 'class1', 'class2', 'class2', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class1', 'class3', 'class1', 'class2', 'class1', 'class3', 'class1', 'class2', 'class1', 'class2', 'class2', 'class1', 'class1', 'class1', 'class3', 'class1', 'class2', 'class2', 'class1', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class3', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class3', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class3', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class1', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class1', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class1', 'class2', 'class2', 'class2', 'class3', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class3', 'class1', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class3', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class3', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class1', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class1', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class1', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class1', 'class2', 'class3', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class2', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class2', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class2', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class2', 'class3', 'class3', 'class3', 'class3', 'class3', 'class2', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class1', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class2', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class2', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class2', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class2', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class2', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3', 'class3']\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "# Predict classes for test set\n",
    "predictions = model.predict(test_generator)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Map predictions to class labels\n",
    "class_names = list(train_generator.class_indices.keys())\n",
    "predicted_labels = [class_names[i] for i in predicted_classes]\n",
    "\n",
    "# Print the predictions\n",
    "print(predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cc39851-c6bc-4fb1-b538-37e1821bb28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save('resnet50_image_classifier.h5')\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33acf98e-93f2-4029-a835-a63b2aaf8eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Accuracy and Loss Curves\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('accuracy_loss_curves.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9e88112-aeff-4a67-80a7-2c908aaa515a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 2s/step\n"
     ]
    }
   ],
   "source": [
    "# 2. Confusion Matrix\n",
    "Y_pred = model.predict(test_generator)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "class_names = list(train_generator.class_indices.keys())\n",
    "cm = confusion_matrix(test_generator.classes, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "108e3b99-b3bf-489b-ba1d-6d428585a123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      class1       0.98      0.68      0.80       588\n",
      "      class2       0.76      0.97      0.85       574\n",
      "      class3       0.96      0.99      0.97       696\n",
      "\n",
      "    accuracy                           0.88      1858\n",
      "   macro avg       0.90      0.88      0.87      1858\n",
      "weighted avg       0.90      0.88      0.88      1858\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_generator.classes, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35cede66-9e77-46ad-b0f3-bec025f8375f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)          │      <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100352</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │      <span style=\"color: #00af00; text-decoration-color: #00af00\">25,690,368</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">771</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m2048\u001b[0m)          │      \u001b[38;5;34m23,587,712\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100352\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │      \u001b[38;5;34m25,690,368\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                   │             \u001b[38;5;34m771\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">100,661,131</span> (383.99 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m100,661,131\u001b[0m (383.99 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,691,139</span> (98.00 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,691,139\u001b[0m (98.00 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> (89.98 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,587,712\u001b[0m (89.98 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">51,382,280</span> (196.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m51,382,280\u001b[0m (196.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4. Model Summary\n",
    "print(\"\\nModel Summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d58bf115-eb63-4d9d-b05d-8f74bb910cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Feature map shape: (1, 230, 230, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Assuming model is already defined and loaded\n",
    "# Correcting the layer selection to point to the first convolutional layer\n",
    "first_conv_layer = None\n",
    "for layer in model.layers[0].layers:\n",
    "    if 'conv' in layer.name:\n",
    "        first_conv_layer = layer\n",
    "        break\n",
    "\n",
    "# Check if the correct layer is found\n",
    "if first_conv_layer is None:\n",
    "    raise ValueError(\"No convolutional layer found in the model.\")\n",
    "\n",
    "# Define the feature map model\n",
    "feature_maps = tf.keras.models.Model(inputs=model.layers[0].inputs, outputs=first_conv_layer.output)\n",
    "\n",
    "# Load and preprocess the image\n",
    "img = next(test_generator)[0][0]\n",
    "img = np.expand_dims(img, axis=0)\n",
    "\n",
    "# Get the feature map\n",
    "feature_map = feature_maps.predict(img)\n",
    "\n",
    "# Check the shape of the feature map\n",
    "print(\"Feature map shape:\", feature_map.shape)  # Debugging line\n",
    "\n",
    "# Plot the feature maps\n",
    "num_filters = feature_map.shape[-1]\n",
    "fig, axes = plt.subplots(4, 8, figsize=(20, 10))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < num_filters:  # Ensure we don't index out of bounds\n",
    "        ax.imshow(feature_map[0, :, :, i], cmap='viridis')\n",
    "    ax.axis('off')\n",
    "plt.suptitle(\"Feature Maps of First Convolutional Layer\")\n",
    "plt.savefig('feature_maps.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c0bb6323-9fc3-46d4-8ed7-5a427046896f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphs and performance metrics have been saved.\n"
     ]
    }
   ],
   "source": [
    "# Save model performance metrics\n",
    "with open('model_performance.txt', 'w') as f:\n",
    "    f.write(f\"Final Training Accuracy: {history.history['accuracy'][-1]:.4f}\\n\")\n",
    "    f.write(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\\n\")\n",
    "    f.write(f\"Final Training Loss: {history.history['loss'][-1]:.4f}\\n\")\n",
    "    f.write(f\"Final Validation Loss: {history.history['val_loss'][-1]:.4f}\\n\")\n",
    "    f.write(\"\\nClassification Report:\\n\")\n",
    "    f.write(classification_report(test_generator.classes, y_pred, target_names=class_names))\n",
    "\n",
    "print(\"Graphs and performance metrics have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0fc5d854-def4-4d40-a007-a29c26ef540f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load and preprocess a single image for prediction\n",
    "def load_and_preprocess_image(img_path, target_size=(224, 224)):\n",
    "    img = image.load_img(img_path, target_size=target_size)\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    img_array = img_array / 255.0  # Normalize to [0, 1] range\n",
    "    return img_array\n",
    "\n",
    "# Predict the class of an image using the loaded model\n",
    "def predict_image(model, img_path, class_indices, threshold=0.5):\n",
    "    img_array = load_and_preprocess_image(img_path)\n",
    "    prediction = model.predict(img_array)\n",
    "    predicted_class_index = np.argmax(prediction, axis=1)[0]\n",
    "    confidence = prediction[0][predicted_class_index]\n",
    "\n",
    "    if confidence < threshold:\n",
    "        return \"unknown\", confidence\n",
    "\n",
    "    class_labels = {v: k for k, v in class_indices.items()}\n",
    "    predicted_class_label = class_labels[predicted_class_index]\n",
    "    return predicted_class_label, confidence\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = tf.keras.models.load_model('resnet50_image_classifier.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c990d793-6313-47b4-91a1-6423ea3fd46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 575ms/step\n",
      "The predicted class is: class2 with confidence 1.00\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "img_path = \"Class_2.jpg\"\n",
    "predicted_class, confidence = predict_image(loaded_model, img_path, train_generator.class_indices)\n",
    "\n",
    "if predicted_class == \"unknown\":\n",
    "    print(\"The image does not belong to any of the known classes.\")\n",
    "else:\n",
    "    print(f\"The predicted class is: {predicted_class} with confidence {confidence:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a32761-5f4f-4b64-8b54-ca7b9f548dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333bfa55-d83f-412d-8e0a-e4ed631c01db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
